"""ml10.py â€” A tiny, self-contained Python module implementing 10 classic ML algorithmswith a UNIFIED request/response interface suitable for backend services.Algorithms included (10 total):1) DecisionTreeClassifier (CART, Gini)2) LinearSVMClassifier (multi-class OVR, SGD-optimized hinge loss)3) KNNClassifier4) GaussianNaiveBayes5) LogisticRegressionClassifier (multi-class OVR)6) MLPClassifier (1-hidden-layer, softmax)7) LinearRegressionModel (OLS with optional L2)8) MLPRegressor (1-hidden-layer, MSE)9) KMeansClustering10) PCA (via SVD; predict() == transform())Unified API (stateless):- serve_request(payload: dict) -> dict  payload = {    "algo": str,                    # one of the keys in ALGORITHMS below    "action": "train"|"infer"|"score",    "X": list[list[float]],         # features (n_samples x n_features)    "y": list | None,               # targets/labels when needed    "params": dict | None,          # algo hyperparams    "state": dict | None            # previously returned model state for "infer"/"score"  }Return:- {    "ok": bool,    "algo": str,    "task_type": "classification"|"regression"|"unsupervised"|"dimensionality_reduction",    "action": str,    "y_pred": list | None,          # predictions (labels / values). For PCA: transformed features as list[list]    "y_proba": list[list] | None,   # for classifiers that can output probabilities (if available)    "metrics": dict | None,         # accuracy / MAE / MSE / inertia etc.    "state": dict | None,           # serializable model parameters    "message": str | None  }Notes:- No third-party ML libs are used; only numpy.- All states (weights, thresholds, prototypes) are JSON-serializable.- This module is intentionally compact and educational; not optimized for big data."""from __future__ import annotationsfrom dataclasses import dataclassfrom typing import Any, Dict, Optional, Tupleimport mathimport randomimport numpy as np# ------------ Utilities ------------def to_numpy(X):    X = np.asarray(X, dtype=float)    if X.ndim == 1:        X = X.reshape(-1, 1)    return Xdef to_numpy_1d(y):    if y is None:        return None    y = np.asarray(y)    return y.astype(int) if y.dtype.kind in "iu" else y.astype(float)def one_hot(y: np.ndarray, num_classes: int) -> np.ndarray:    Y = np.zeros((y.shape[0], num_classes), dtype=float)    Y[np.arange(y.shape[0]), y] = 1.0    return Ydef softmax(z: np.ndarray) -> np.ndarray:    z = z - np.max(z, axis=1, keepdims=True)    e = np.exp(z)    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)def relu(x): return np.maximum(0.0, x)def relu_grad(x): return (x > 0).astype(float)def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))def sigmoid_grad(z):     s = sigmoid(z);     return s * (1.0 - s)# ---- Metrics ----def accuracy(y_true, y_pred) -> float:    y_true = to_numpy_1d(y_true).ravel().astype(int)    y_pred = to_numpy_1d(y_pred).ravel().astype(int)    return float(np.mean(y_true == y_pred)) if len(y_true) else 0.0def mse(y_true, y_pred) -> float:    y_true = to_numpy_1d(y_true).ravel().astype(float)    y_pred = to_numpy_1d(y_pred).ravel().astype(float)    return float(np.mean((y_true - y_pred) ** 2)) if len(y_true) else 0.0def mae(y_true, y_pred) -> float:    y_true = to_numpy_1d(y_true).ravel().astype(float)    y_pred = to_numpy_1d(y_pred).ravel().astype(float)    return float(np.mean(np.abs(y_true - y_pred))) if len(y_true) else 0.0# ------------ Base Class ------------class BaseModel:    task_type: str = "base"    name: str = "base"    def fit(self, X, y=None): raise NotImplementedError    def predict(self, X): raise NotImplementedError    def predict_proba(self, X): return None    def get_state(self) -> Dict[str, Any]: return {}    def set_state(self, state: Dict[str, Any]): return self    def get_params(self) -> Dict[str, Any]: return {}    def set_params(self, **params): return self# ------------ 1) Decision Tree (CART) Classifier ------------class DecisionTreeClassifier(BaseModel):    task_type = "classification"    name = "decision_tree_clf"    def __init__(self, max_depth: int = 10, min_samples_split: int = 2, n_features: Optional[int] = None):        self.max_depth = max_depth        self.min_samples_split = min_samples_split        self.n_features = n_features  # if None use all        self.tree_ = None    def _gini(self, y):        m = len(y)        if m == 0: return 0.0        _, counts = np.unique(y, return_counts=True)        p = counts / m        return 1.0 - np.sum(p**2)    def _best_split(self, X, y):        m, n = X.shape        features = range(n) if self.n_features is None else np.random.choice(n, self.n_features, replace=False)        best = {"gini": 1.0, "idx": None, "thr": None}        for idx in features:            thresholds = np.unique(X[:, idx])            for thr in thresholds:                left = y[X[:, idx] <= thr]                right = y[X[:, idx] > thr]                g = (len(left)*self._gini(left) + len(right)*self._gini(right)) / m                if g < best["gini"]:                    best = {"gini": g, "idx": int(idx), "thr": float(thr)}        return best["idx"], best["thr"]    def _build(self, X, y, depth=0):        num_samples_per_class = [np.sum(y == i) for i in np.unique(y)]        predicted_class = int(np.argmax(num_samples_per_class))        node = {"type":"leaf", "class": predicted_class}        if depth < self.max_depth and len(y) >= self.min_samples_split and self._gini(y) > 0.0:            idx, thr = self._best_split(X, y)            if idx is not None:                left_idx = X[:, idx] <= thr                node = {                    "type":"node",                    "idx": int(idx),                    "thr": float(thr),                    "left": self._build(X[left_idx], y[left_idx], depth+1),                    "right": self._build(X[~left_idx], y[~left_idx], depth+1)                }        return node    def fit(self, X, y):        X, y = to_numpy(X), to_numpy_1d(y).astype(int)        self.tree_ = self._build(X, y, depth=0)        return self    def _predict_one(self, x, node):        if node["type"] == "leaf":            return node["class"]        return self._predict_one(x, node["left"] if x[node["idx"]] <= node["thr"] else node["right"])    def predict(self, X):        X = to_numpy(X)        return np.array([self._predict_one(x, self.tree_) for x in X], dtype=int)    def get_state(self):         return {"max_depth": self.max_depth, "min_samples_split": self.min_samples_split,                "n_features": self.n_features, "tree": self.tree_}    def set_state(self, state):        self.max_depth = state.get("max_depth", 10)        self.min_samples_split = state.get("min_samples_split", 2)        self.n_features = state.get("n_features", None)        self.tree_ = state.get("tree", None)        return self# ------------ 2) Linear SVM Classifier (OVR, SGD hinge) ------------class LinearSVMClassifier(BaseModel):    task_type = "classification"    name = "svm_clf"    def __init__(self, lr: float = 1e-2, epochs: int = 200, C: float = 1.0, random_state: int = 42):        self.lr = lr        self.epochs = epochs        self.C = C        self.random_state = random_state        self.W = None  # (n_features, n_classes)        self.b = None    def fit(self, X, y):        X, y = to_numpy(X), to_numpy_1d(y).astype(int)        n, d = X.shape        classes = np.unique(y)        k = len(classes)        rng = np.random.default_rng(self.random_state)        self.W = np.zeros((d, k))        self.b = np.zeros(k)        for epoch in range(self.epochs):            idx = rng.permutation(n)            for i in idx:                xi, yi = X[i], y[i]                scores = xi @ self.W + self.b                margins = 1 - scores + scores[yi]                margins[yi] = 0.0                j = np.argmax(margins)                # if margin violated for class j                if 1 - (scores[yi] - scores[j]) > 0:                    # gradient update                    self.W[:, yi] += self.lr * (self.C * xi)                    self.b[yi] += self.lr * self.C                    self.W[:, j] -= self.lr * (self.C * xi)                    self.b[j] -= self.lr * self.C                # regularize (L2)                self.W *= (1 - self.lr)        return self    def predict(self, X):        X = to_numpy(X)        scores = X @ self.W + self.b        return np.argmax(scores, axis=1)    def get_state(self):         return {"lr": self.lr, "epochs": self.epochs, "C": self.C, "random_state": self.random_state,                "W": self.W.tolist() if self.W is not None else None,                "b": self.b.tolist() if self.b is not None else None}    def set_state(self, state):        self.lr = state.get("lr", 1e-2); self.epochs = state.get("epochs", 200)        self.C = state.get("C", 1.0); self.random_state = state.get("random_state", 42)        self.W = np.array(state.get("W")) if state.get("W") is not None else None        self.b = np.array(state.get("b")) if state.get("b") is not None else None        return self# ------------ 3) KNN Classifier ------------class KNNClassifier(BaseModel):    task_type = "classification"    name = "knn_clf"    def __init__(self, k: int = 5, weighted: bool = False):        self.k = k        self.weighted = weighted        self.X = None        self.y = None    def fit(self, X, y):        self.X = to_numpy(X)        self.y = to_numpy_1d(y).astype(int)        return self    def predict(self, X):        X = to_numpy(X)        preds = []        for xi in X:            dists = np.linalg.norm(self.X - xi, axis=1)            idx = np.argsort(dists)[:self.k]            votes = self.y[idx]            if self.weighted:                w = 1.0 / (dists[idx] + 1e-9)                scores = {}                for v, ww in zip(votes, w):                    scores[v] = scores.get(v, 0.0) + ww                preds.append(max(scores.items(), key=lambda kv: kv[1])[0])            else:                preds.append(np.bincount(votes).argmax())        return np.array(preds, dtype=int)    def get_state(self):        return {"k": self.k, "weighted": self.weighted,                "X": self.X.tolist() if self.X is not None else None,                "y": self.y.tolist() if self.y is not None else None}    def set_state(self, state):        self.k = state.get("k", 5); self.weighted = state.get("weighted", False)        self.X = np.array(state.get("X")) if state.get("X") is not None else None        self.y = np.array(state.get("y")) if state.get("y") is not None else None        return self# ------------ 4) Gaussian Naive Bayes ------------class GaussianNaiveBayes(BaseModel):    task_type = "classification"    name = "gaussian_nb"    def __init__(self, eps: float = 1e-9):        self.eps = eps        self.classes_ = None        self.prior_ = None        self.mean_ = None        self.var_ = None    def fit(self, X, y):        X, y = to_numpy(X), to_numpy_1d(y).astype(int)        self.classes_ = np.unique(y)        k = len(self.classes_)        n_features = X.shape[1]        self.mean_ = np.zeros((k, n_features))        self.var_ = np.zeros((k, n_features))        self.prior_ = np.zeros(k)        for i, c in enumerate(self.classes_):            Xc = X[y == c]            self.mean_[i] = Xc.mean(axis=0)            self.var_[i] = Xc.var(axis=0) + self.eps            self.prior_[i] = Xc.shape[0] / X.shape[0]        return self    def _log_gauss(self, i, X):        mean, var = self.mean_[i], self.var_[i]        return -0.5 * np.sum(np.log(2*np.pi*var) + ((X - mean)**2)/var, axis=1)    def predict_proba(self, X):        X = to_numpy(X)        log_probs = []        for i in range(len(self.classes_)):            log_probs.append(self._log_gauss(i, X) + np.log(self.prior_[i] + self.eps))        log_probs = np.vstack(log_probs).T        # normalize        log_probs -= log_probs.max(axis=1, keepdims=True)        probs = np.exp(log_probs)        probs /= probs.sum(axis=1, keepdims=True)        return probs    def predict(self, X):        return np.argmax(self.predict_proba(X), axis=1)    def get_state(self):        return {"eps": self.eps,                "classes": self.classes_.tolist() if self.classes_ is not None else None,                "prior": self.prior_.tolist() if self.prior_ is not None else None,                "mean": self.mean_.tolist() if self.mean_ is not None else None,                "var": self.var_.tolist() if self.var_ is not None else None}    def set_state(self, state):        self.eps = state.get("eps", 1e-9)        self.classes_ = np.array(state.get("classes")) if state.get("classes") is not None else None        self.prior_ = np.array(state.get("prior")) if state.get("prior") is not None else None        self.mean_ = np.array(state.get("mean")) if state.get("mean") is not None else None        self.var_ = np.array(state.get("var")) if state.get("var") is not None else None        return self# ------------ 5) Logistic Regression (OVR) ------------class LogisticRegressionClassifier(BaseModel):    task_type = "classification"    name = "logreg_clf"    def __init__(self, lr: float = 0.1, epochs: int = 200, l2: float = 0.0):        self.lr = lr; self.epochs = epochs; self.l2 = l2        self.W = None; self.b = None    def fit(self, X, y):        X, y = to_numpy(X), to_numpy_1d(y).astype(int)        n, d = X.shape; classes = np.unique(y); k = len(classes)        self.W = np.zeros((d, k)); self.b = np.zeros(k)        Y = one_hot(y, k)        for _ in range(self.epochs):            logits = X @ self.W + self.b            P = softmax(logits)            grad_W = X.T @ (P - Y) / n + self.l2 * self.W            grad_b = np.mean(P - Y, axis=0)            self.W -= self.lr * grad_W            self.b -= self.lr * grad_b        return self    def predict_proba(self, X):        X = to_numpy(X)        return softmax(X @ self.W + self.b)    def predict(self, X):        return np.argmax(self.predict_proba(X), axis=1)    def get_state(self):        return {"lr": self.lr, "epochs": self.epochs, "l2": self.l2,                "W": self.W.tolist() if self.W is not None else None,                "b": self.b.tolist() if self.b is not None else None}    def set_state(self, state):        self.lr = state.get("lr", 0.1); self.epochs = state.get("epochs", 200); self.l2 = state.get("l2", 0.0)        self.W = np.array(state.get("W")) if state.get("W") is not None else None        self.b = np.array(state.get("b")) if state.get("b") is not None else None        return self# ------------ 6) MLP Classifier ------------class MLPClassifier(BaseModel):    task_type = "classification"    name = "mlp_clf"    def __init__(self, hidden: int = 64, lr: float = 0.1, epochs: int = 200, batch_size: int = 32, random_state: int = 42):        self.hidden = hidden; self.lr = lr; self.epochs = epochs; self.batch_size = batch_size        self.random_state = random_state        self.W1 = None; self.b1 = None; self.W2 = None; self.b2 = None    def fit(self, X, y):        X, y = to_numpy(X), to_numpy_1d(y).astype(int)        n, d = X.shape; k = len(np.unique(y))        rng = np.random.default_rng(self.random_state)        self.W1 = rng.normal(scale=0.01, size=(d, self.hidden)); self.b1 = np.zeros(self.hidden)        self.W2 = rng.normal(scale=0.01, size=(self.hidden, k)); self.b2 = np.zeros(k)        Y = one_hot(y, k)        for _ in range(self.epochs):            # mini-batch            idx = rng.permutation(n)            for i in range(0, n, self.batch_size):                batch = idx[i:i+self.batch_size]                Xb, Yb = X[batch], Y[batch]                h_pre = Xb @ self.W1 + self.b1                h = relu(h_pre)                logits = h @ self.W2 + self.b2                P = softmax(logits)                # backprop                dlogits = (P - Yb) / Xb.shape[0]                dW2 = h.T @ dlogits                db2 = dlogits.mean(axis=0) * Xb.shape[0]                dh = dlogits @ self.W2.T                dh_pre = dh * relu_grad(h_pre)                dW1 = Xb.T @ dh_pre                db1 = dh_pre.mean(axis=0) * Xb.shape[0]                # update                self.W2 -= self.lr * dW2; self.b2 -= self.lr * db2                self.W1 -= self.lr * dW1; self.b1 -= self.lr * db1        return self    def predict_proba(self, X):        X = to_numpy(X)        h = relu(X @ self.W1 + self.b1)        return softmax(h @ self.W2 + self.b2)    def predict(self, X):        return np.argmax(self.predict_proba(X), axis=1)    def get_state(self):        s = {"hidden": self.hidden, "lr": self.lr, "epochs": self.epochs, "batch_size": self.batch_size,             "random_state": self.random_state,             "W1": self.W1.tolist() if self.W1 is not None else None,             "b1": self.b1.tolist() if self.b1 is not None else None,             "W2": self.W2.tolist() if self.W2 is not None else None,             "b2": self.b2.tolist() if self.b2 is not None else None}        return s    def set_state(self, state):        self.hidden = state.get("hidden", 64); self.lr = state.get("lr", 0.1)        self.epochs = state.get("epochs", 200); self.batch_size = state.get("batch_size", 32)        self.random_state = state.get("random_state", 42)        self.W1 = np.array(state.get("W1")) if state.get("W1") is not None else None        self.b1 = np.array(state.get("b1")) if state.get("b1") is not None else None        self.W2 = np.array(state.get("W2")) if state.get("W2") is not None else None        self.b2 = np.array(state.get("b2")) if state.get("b2") is not None else None        return self# ------------ 7) Linear Regression (OLS + optional Ridge) ------------class LinearRegressionModel(BaseModel):    task_type = "regression"    name = "linear_reg"    def __init__(self, l2: float = 0.0):        self.l2 = l2        self.W = None  # (d,)        self.b = 0.0    def fit(self, X, y):        X = to_numpy(X); y = to_numpy_1d(y).astype(float)        n, d = X.shape        X_ext = np.hstack([X, np.ones((n,1))])        A = X_ext.T @ X_ext        if self.l2 > 0:            reg = self.l2 * np.eye(d+1)            reg[-1, -1] = 0.0  # don't regularize bias            A = A + reg        w_ext = np.linalg.pinv(A) @ (X_ext.T @ y)        self.W = w_ext[:-1]; self.b = w_ext[-1]        return self    def predict(self, X):        X = to_numpy(X)        return X @ self.W + self.b    def get_state(self):        return {"l2": self.l2, "W": self.W.tolist() if self.W is not None else None, "b": float(self.b)}    def set_state(self, state):        self.l2 = state.get("l2", 0.0)        self.W = np.array(state.get("W")) if state.get("W") is not None else None        self.b = float(state.get("b", 0.0))        return self# ------------ 8) MLP Regressor ------------class MLPRegressor(BaseModel):    task_type = "regression"    name = "mlp_reg"    def __init__(self, hidden: int = 64, lr: float = 0.01, epochs: int = 300, batch_size: int = 32, random_state: int = 42):        self.hidden = hidden; self.lr = lr; self.epochs = epochs; self.batch_size = batch_size        self.random_state = random_state        self.W1 = None; self.b1 = None; self.W2 = None; self.b2 = None    def fit(self, X, y):        X = to_numpy(X); y = to_numpy_1d(y).reshape(-1,1).astype(float)        n, d = X.shape        rng = np.random.default_rng(self.random_state)        self.W1 = rng.normal(scale=0.01, size=(d, self.hidden)); self.b1 = np.zeros(self.hidden)        self.W2 = rng.normal(scale=0.01, size=(self.hidden, 1)); self.b2 = np.zeros(1)        for _ in range(self.epochs):            idx = rng.permutation(n)            for i in range(0, n, self.batch_size):                b = idx[i:i+self.batch_size]                Xb, yb = X[b], y[b]                h_pre = Xb @ self.W1 + self.b1                h = relu(h_pre)                yhat = h @ self.W2 + self.b2                err = (yhat - yb)                dW2 = h.T @ err / Xb.shape[0]                db2 = err.mean(axis=0) * Xb.shape[0]                dh = err @ self.W2.T                dh_pre = dh * relu_grad(h_pre)                dW1 = Xb.T @ dh_pre / Xb.shape[0]                db1 = dh_pre.mean(axis=0) * Xb.shape[0]                self.W2 -= self.lr * dW2; self.b2 -= self.lr * db2                self.W1 -= self.lr * dW1; self.b1 -= self.lr * db1        return self    def predict(self, X):        X = to_numpy(X)        h = relu(X @ self.W1 + self.b1)        return (h @ self.W2 + self.b2).ravel()    def get_state(self):        return {"hidden": self.hidden, "lr": self.lr, "epochs": self.epochs, "batch_size": self.batch_size,                "random_state": self.random_state,                "W1": self.W1.tolist() if self.W1 is not None else None,                "b1": self.b1.tolist() if self.b1 is not None else None,                "W2": self.W2.tolist() if self.W2 is not None else None,                "b2": self.b2.tolist() if self.b2 is not None else None}    def set_state(self, state):        self.hidden = state.get("hidden", 64); self.lr = state.get("lr", 0.01)        self.epochs = state.get("epochs", 300); self.batch_size = state.get("batch_size", 32)        self.random_state = state.get("random_state", 42)        self.W1 = np.array(state.get("W1")) if state.get("W1") is not None else None        self.b1 = np.array(state.get("b1")) if state.get("b1") is not None else None        self.W2 = np.array(state.get("W2")) if state.get("W2") is not None else None        self.b2 = np.array(state.get("b2")) if state.get("b2") is not None else None        return self# ------------ 9) K-Means Clustering ------------class KMeansClustering(BaseModel):    task_type = "unsupervised"    name = "kmeans"    def __init__(self, k: int = 3, max_iter: int = 200, tol: float = 1e-4, random_state: int = 42):        self.k = k; self.max_iter = max_iter; self.tol = tol; self.random_state = random_state        self.centers_ = None; self.inertia_ = None    def fit(self, X, y=None):        X = to_numpy(X)        n, d = X.shape        rng = np.random.default_rng(self.random_state)        idx = rng.choice(n, self.k, replace=False)        centers = X[idx].copy()        for _ in range(self.max_iter):            # assign            dists = np.linalg.norm(X[:, None, :] - centers[None, :, :], axis=2)            labels = np.argmin(dists, axis=1)            new_centers = np.array([X[labels == i].mean(axis=0) if np.any(labels == i) else centers[i] for i in range(self.k)])            shift = np.linalg.norm(new_centers - centers)            centers = new_centers            if shift < self.tol: break        self.centers_ = centers        self.inertia_ = float(((X - centers[labels])**2).sum())        return self    def predict(self, X):        X = to_numpy(X)        dists = np.linalg.norm(X[:, None, :] - self.centers_[None, :, :], axis=2)        return np.argmin(dists, axis=1)    def get_state(self):        return {"k": self.k, "max_iter": self.max_iter, "tol": self.tol, "random_state": self.random_state,                "centers": self.centers_.tolist() if self.centers_ is not None else None,                "inertia": self.inertia_}    def set_state(self, state):        self.k = state.get("k", 3); self.max_iter = state.get("max_iter", 200)        self.tol = state.get("tol", 1e-4); self.random_state = state.get("random_state", 42)        self.centers_ = np.array(state.get("centers")) if state.get("centers") is not None else None        self.inertia_ = state.get("inertia")        return self# ------------ 10) PCA ------------class PCA(BaseModel):    task_type = "dimensionality_reduction"    name = "pca"    def __init__(self, n_components: int = 2, whiten: bool = False):        self.n_components = n_components; self.whiten = whiten        self.mean_ = None; self.components_ = None; self.var_ = None    def fit(self, X, y=None):        X = to_numpy(X)        self.mean_ = X.mean(axis=0)        Xc = X - self.mean_        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)        self.components_ = Vt[:self.n_components]        self.var_ = (S**2) / (X.shape[0]-1)        return self    def transform(self, X):        X = to_numpy(X) - self.mean_        Z = X @ self.components_.T        if self.whiten:            comp_var = self.var_[:self.n_components]            Z = Z / np.sqrt(comp_var + 1e-9)        return Z    def predict(self, X):        # for unified API, predict() returns transformed features        return self.transform(X)    def get_state(self):        return {"n_components": self.n_components, "whiten": self.whiten,                "mean": self.mean_.tolist() if self.mean_ is not None else None,                "components": self.components_.tolist() if self.components_ is not None else None,                "var": self.var_.tolist() if self.var_ is not None else None}    def set_state(self, state):        self.n_components = state.get("n_components", 2); self.whiten = state.get("whiten", False)        self.mean_ = np.array(state.get("mean")) if state.get("mean") is not None else None        self.components_ = np.array(state.get("components")) if state.get("components") is not None else None        self.var_ = np.array(state.get("var")) if state.get("var") is not None else None        return self# ------------ Registry, factory & service ------------ALGORITHMS = {    # classification    "decision_tree_clf": DecisionTreeClassifier,    "svm_clf": LinearSVMClassifier,    "knn_clf": KNNClassifier,    "gaussian_nb": GaussianNaiveBayes,    "logreg_clf": LogisticRegressionClassifier,    "mlp_clf": MLPClassifier,    # regression    "linear_reg": LinearRegressionModel,    "mlp_reg": MLPRegressor,    # unsupervised    "kmeans": KMeansClustering,    "pca": PCA,}def _build_model(algo: str, params: Optional[Dict[str, Any]] = None, state: Optional[Dict[str, Any]] = None) -> BaseModel:    if algo not in ALGORITHMS:        raise ValueError(f"Unknown algo '{algo}'. Valid: {list(ALGORITHMS.keys())}")    cls = ALGORITHMS[algo]    model = cls(**(params or {}))    if state:        model.set_state(state)    return modeldef serve_request(payload: Dict[str, Any]) -> Dict[str, Any]:    """    Stateless service entry. Train / Infer / Score with a unified JSON payload.    """    try:        algo = payload["algo"]        action = payload["action"]        X = payload.get("X", None)        y = payload.get("y", None)        params = payload.get("params", None)        state = payload.get("state", None)        if X is None:            return {"ok": False, "algo": algo, "action": action, "message": "X is required"}        model = _build_model(algo, params, state)        task_type = getattr(model, "task_type", "unknown")        y_pred = None; y_proba = None; metrics = None; new_state = None        if action == "train":            model.fit(X, y)            # after training, we can report quick training metrics if y exists            if task_type == "classification" and y is not None:                y_pred = model.predict(X).tolist()                metrics = {"accuracy": accuracy(y, y_pred)}            elif task_type == "regression" and y is not None:                y_pred = model.predict(X).tolist()                metrics = {"mse": mse(y, y_pred), "mae": mae(y, y_pred)}            elif task_type == "unsupervised":                if hasattr(model, "inertia_"):                    metrics = {"inertia": getattr(model, "inertia_")}            new_state = model.get_state()        elif action == "infer":            # require state (model previously trained)            if state is None:                return {"ok": False, "algo": algo, "action": action, "message": "state is required for infer"}            y_pred = model.predict(X)            if isinstance(y_pred, np.ndarray) and y_pred.ndim == 2:                y_pred = y_pred.tolist()  # e.g., PCA transform            else:                y_pred = to_numpy_1d(y_pred).tolist()            try:                proba = model.predict_proba(X)                if proba is not None:                    y_proba = proba.tolist()            except Exception:                y_proba = None        elif action == "score":            if y is None:                return {"ok": False, "algo": algo, "action": action, "message": "y is required for score"}            if state is None:                return {"ok": False, "algo": algo, "action": action, "message": "state is required for score"}            y_hat = _build_model(algo, params, state).predict(X)            if task_type == "classification":                metrics = {"accuracy": accuracy(y, y_hat)}            elif task_type == "regression":                metrics = {"mse": mse(y, y_hat), "mae": mae(y, y_hat)}            else:                metrics = {}        else:            return {"ok": False, "algo": algo, "action": action, "message": f"Unknown action {action}"}        return {"ok": True, "algo": algo, "task_type": task_type, "action": action,                "y_pred": y_pred, "y_proba": y_proba, "metrics": metrics, "state": new_state, "message": None}    except Exception as e:        return {"ok": False, "algo": payload.get("algo"), "action": payload.get("action"),                "message": f"{type(e).__name__}: {str(e)}"}# -------------- Tiny self-test (optional) --------------if __name__ == "__main__":    # A quick smoke test on a toy dataset (2 classes)    X = [[0,0],[0,1],[1,0],[1,1],[2,2],[2,3],[3,2],[3,3]]    y = [0,0,0,0,1,1,1,1]    req = {"algo": "logreg_clf", "action": "train", "X": X, "y": y, "params": {"epochs": 300, "lr": 0.2}}    out = serve_request(req)    print("Train:", out["metrics"])    state = out["state"]    inf = serve_request({"algo":"logreg_clf","action":"infer","X":[[1.5,1.5],[0.2,0.1]],"state":state})    print("Infer:", inf["y_pred"])